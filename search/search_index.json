{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"[DRAFT] A Senseable Python Template View on GitHub Motivation This guide provides recommendations for how to structure Python projects developed within the MIT Senseable City Lab, and has the following goals in mind: Remove friction to get started with Python projects Improve internal code reuse and workflow reproducibility Centralize access to shared resources Make it easier to publish project code publicly Overview This guide strongly recommends that the cookiecutter-data-science template be used to create the skeleton for your project. Using this template will create an initial project structure that can be shared among projects in the lab, but can also be modified if needed. This template is implemented using Cookiecutter , a command line utility to quickly build out your project structure. There are many different cookiecutter templates that can be used for different project setups with different focuses and strengths. We specifically recommend the template cookiecutter-data-science because it is widely adopted for use in research environments and presents an easy-to-share and flexible starting off point. As stated in their documentation: A logical, flexible, and reasonably standardized project structure for doing and sharing data science work. It is recommended that you read about the opinions reflected in the template , and use them to guide your own work and to inform any choice to deviate from the recommended structure. Resulting Directory Structure This is the directory structure that is created after completing the setup with this template. \u251c\u2500\u2500 LICENSE <- Open-source license if one is chosen \u251c\u2500\u2500 Makefile <- Makefile with convenience commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default mkdocs project; see www.mkdocs.org for details \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 pyproject.toml <- Project configuration file with package metadata for \u2502 {{ cookiecutter.module_name }} and configuration for tools like black \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 requirements.txt <- The requirements file for reproducing the analysis environment, e.g. \u2502 generated with `pip freeze > requirements.txt` \u2502 \u251c\u2500\u2500 setup.cfg <- Configuration file for flake8 \u2502 \u2514\u2500\u2500 {{ cookiecutter.module_name }} <- Source code for use in this project. \u2502 \u251c\u2500\u2500 __init__.py <- Makes {{ cookiecutter.module_name }} a Python module \u2502 \u251c\u2500\u2500 config.py <- Store useful variables and configuration \u2502 \u251c\u2500\u2500 dataset.py <- Scripts to download or generate data \u2502 \u251c\u2500\u2500 features.py <- Code to create features for modeling \u2502 \u251c\u2500\u2500 modeling \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 predict.py <- Code to run model inference with trained models \u2502 \u2514\u2500\u2500 train.py <- Code to train models \u2502 \u2514\u2500\u2500 plots.py <- Code to create visualizations","title":"Home"},{"location":"#draft-a-senseable-python-template","text":"View on GitHub","title":"[DRAFT] A Senseable Python Template"},{"location":"#motivation","text":"This guide provides recommendations for how to structure Python projects developed within the MIT Senseable City Lab, and has the following goals in mind: Remove friction to get started with Python projects Improve internal code reuse and workflow reproducibility Centralize access to shared resources Make it easier to publish project code publicly","title":"Motivation"},{"location":"#overview","text":"This guide strongly recommends that the cookiecutter-data-science template be used to create the skeleton for your project. Using this template will create an initial project structure that can be shared among projects in the lab, but can also be modified if needed. This template is implemented using Cookiecutter , a command line utility to quickly build out your project structure. There are many different cookiecutter templates that can be used for different project setups with different focuses and strengths. We specifically recommend the template cookiecutter-data-science because it is widely adopted for use in research environments and presents an easy-to-share and flexible starting off point. As stated in their documentation: A logical, flexible, and reasonably standardized project structure for doing and sharing data science work. It is recommended that you read about the opinions reflected in the template , and use them to guide your own work and to inform any choice to deviate from the recommended structure.","title":"Overview"},{"location":"#resulting-directory-structure","text":"This is the directory structure that is created after completing the setup with this template. \u251c\u2500\u2500 LICENSE <- Open-source license if one is chosen \u251c\u2500\u2500 Makefile <- Makefile with convenience commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default mkdocs project; see www.mkdocs.org for details \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 pyproject.toml <- Project configuration file with package metadata for \u2502 {{ cookiecutter.module_name }} and configuration for tools like black \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 requirements.txt <- The requirements file for reproducing the analysis environment, e.g. \u2502 generated with `pip freeze > requirements.txt` \u2502 \u251c\u2500\u2500 setup.cfg <- Configuration file for flake8 \u2502 \u2514\u2500\u2500 {{ cookiecutter.module_name }} <- Source code for use in this project. \u2502 \u251c\u2500\u2500 __init__.py <- Makes {{ cookiecutter.module_name }} a Python module \u2502 \u251c\u2500\u2500 config.py <- Store useful variables and configuration \u2502 \u251c\u2500\u2500 dataset.py <- Scripts to download or generate data \u2502 \u251c\u2500\u2500 features.py <- Code to create features for modeling \u2502 \u251c\u2500\u2500 modeling \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 predict.py <- Code to run model inference with trained models \u2502 \u2514\u2500\u2500 train.py <- Code to train models \u2502 \u2514\u2500\u2500 plots.py <- Code to create visualizations","title":"Resulting Directory Structure"},{"location":"getting-started/","text":"Getting Started Prerequisites In order to get started, you'll need the following: A basic understanding of git and a GitHub account. A command line interface to run commands (often Terminal on macOS and Linux, and Command Prompt or PowerShell on Windows). An installation of pipx or pip . Setup Below, we walk through how to quickly set up your new project. This README provides two different setup flows for different expertise and preference profiles. If you would prefer, you are welcome to deviate from these setup flows and make choices as you see fit. More details can be found in the documentation for the cookiecutter-data-science template . Standard Allows you to choose between using pip & venv , or conda for dependency and virtual environment management If you plan to deploy a package, Flit will be the default tool to use. This set up will be best for those who want to use the more traditional requirements.txt file for dependency management, or for users who want more flexibility for more complex projects. Poetic Basic setup with a few extra steps to start using Poetry for all of dependency management, virtual environment generation, and package publication. This setup may be better suited for those who plan to only use Python code and packages in their project. The downside of this set up is that sharing code with other users takes an extra step if they do not have (and do not want to use) Poetry for dependency management. While Poetry can also be used for projects set up with the Standard installation as well, the Poetic set up best takes advantage of the unification that Poetry provides. Install ccds The cookiecutter-data-science template team created a command line tool called ccds to run the setup. With pipx (recommended) pipx install cookiecutter-data-science With pip pip install cookiecutter-data-science Shared Setup Run ccds from the parent directory where you want your project stored # From the parent directory where you want your project ccds Begin filling out the template prompts. These can all be changed later. Enter the project_name Enter the repo_name . Choose a name that has not been used for a repository on your GitHub account. Enter the module_name . If you choose to export your standalone code, this will be the name of the Python package. Enter the author_name . This can be your full name. Enter the description . Enter the python_version_number . As of mid-2024, using a version >=3.9 is recommended. Select the dataset_storage . For large datasets or CSVs, it is recommended they be stored on a cloud storage if possible. If you have details for those cloud storage providers, go ahead and choose the one you use and provide the credentials. If you do not currently use one of those providers but plan to do so eventually, you can select it now and use the default values for the credentials. This will create the convenient scripts in the Makefile , and later when you set up the cloud storage you can modify the details in the Makefile to reflect the relevant credentials (bucket name, etc). Eventually, we will recommend using the SCL Network Attached Storage (NAS) that is hosted in Cambridge for data storage. See the section ] Using the SCL Network Attached Storage (NAS) below for instructions on how to set up the NAS for data storage. If you will use the SCL NAS, select none for dataset_storage . For the remaining choices, choose a setup flow below to follow. Standard Setup Complete template setup Select the environment_manager of your choice. Select none if you wish to set up an environment manager that is not listed. Select the dependency_file of your choice. For pydata_packages , select basic if you would like the following packages added from the start: ipython, jupyterlab, matplotlib, notebook, numpy, pandas, scikit-learn. Select open_source_license . MIT is recommended if the repository will be public. Select docs . If you would prefer to use Read the Docs instead, you can select none and set that up independently. Select code_scaffold . It is recommended to select yes to include some useful boilerplate code. If you are an experienced Python developer, you may prefer to avoid the prebuilt boilerplate and select no . Create a Python virtual environment If you used the Standard setup, you can use the following command to create a new environment using a recipe provided by the template in the Makefile: make create_environment You will then need to activate that environment using the environment manager you've chosen. See Create a Python virtual environment for more information. You can read more about the Makefile in the Working On Your Project section. Poetic Setup Complete template setup For environment_manager , select none . For dependency_file , select requirements.txt . At the end of the setup, we will remove this and use pyproject.toml instead. For pydata_packages , select none . Select open_source_license . MIT is recommended if the repository will be public. Select docs . If you would prefer to use Read the Docs instead, you can select none and set that up independently. Select code_scaffold . It is recommended to select yes to include some useful boilerplate code. If you are an experienced Python developer, you may prefer to avoid the prebuilt boilerplate and select no . Set up Poetry In the Poetic setup flow, we will manage dependencies using Poetry . With Poetry , you only need one tool to cover the following: manage dependencies, create local environments, and publish your package to PyPI. Install Poetry Simple installation with pipx pipx install poetry See detailed instructions here . Initialize Poetry The cookiecutter-data-science template created a pyproject.toml for us already, but we will want to delete a couple sections from that before initiaalizing Poetry . First, delete the sections within the pyproject.toml for [project] and [build-system] . They will probably look something like this: [project] name = \"your-project-name\" version = \"0.0.1\" description = \"the description for your project\" authors = [ { name = \"Deniz Aydemir\" }, ] license = { file = \"LICENSE\" } readme = \"README.md\" classifiers = [ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\" ] requires-python = \"~=3.12\" [build-system] requires = [\"flit_core >=3.2,<4\"] build-backend = \"flit_core.buildapi\" After removing this text from the pyproject.toml , you can now initialize using Poetry . From within the newly created project folder run the following: cd newly-created-project-folder poetry init You will be guided through set up options, and you should chose the same project name you chose during the original set up. Configure in-project virtual environments poetry config virtualenvs.in-project true Install to download dependencies and setup environment poetry install This creates a virtual environment that stored in the project folder at /.venv . This folder is hidden to git thanks to the .gitignore file. If someone else wants to run your code, they can clone or download your repository from GitHub, and run poetry install themselves to have a copy of the virtual environment you are using. Add included dependencies Let's add the helpers that come with the template to the development environment. More information on this is available in Working On Your Project . poetry add black flake8 isort --group dev If you chose yes for the code_scaffolding option, then run the following command to add the packages included in the boilerplate code to the whole project. poetry add python-dotenv loguru typer tqdm Delete the requirements.txt In order to avoid confusion, it is recommended that you delete the requirements.txt file as it will not be used. pyproject.toml It will be useful to familiarize yourself with the pyproject.toml file. This file is the current best practice method for defining project parameters and dependencies. Poetry will populate and use this file as you add dependencies. Other build tools like Flit, Hatch, and Setuptools also use the pyproject.toml . Read more about pyproject.toml at Python's pyproject documentation . What happened to requirements.txt? When using Poetry, all your dependencies are managed in the pyproject.toml file. It is possible to use a requirements.txt to populate your dependencies in your pyproject.toml with Poetry. poetry add $(cat requirements.txt) Similarly, it is possible to export your dependencies from the pyproject.toml to a requirements.txt if needed. # Export dependencies to requirements.txt poetry export -f requirements.txt --output requirements.txt # If you want to include development dependencies as well poetry export -f requirements.txt --output requirements.txt --dev Adjust Makefile In fact, in order to get some of our Makefile recipes working again, we will add those above lines to the requirements recipe. So now the requirements recipe in your Makefile should look like this: ## Install Python Dependencies .PHONY: requirements requirements: poetry export -f requirements.txt --output requirements.txt poetry export -f requirements.txt --output requirements.txt --dev $(PYTHON_INTERPRETER) -m pip install -U pip $(PYTHON_INTERPRETER) -m pip install -r requirements.txt It is important to note that with this change the make requirements recipe will create a requirements.txt based on your Poetry dependencies (both regular and development) based on the pyproject.toml , and install those dependencies using pip . However, if you are using Poetry to manage your dependencies, make requirements should not be used to install dependencies. Instead, the standard poetry install should be used. The Makefile is discussed further in Working With Your Project . Connect to GitHub Whichever setup flow you chose, your final step will be to connect your local folder to a new repo on GitHub. In order to complete this step, ensure you have the GitHub command-line interface (CLI) installed. Then you can run the following commands to connect your local repository to GitHub. # Navigate to your newly created project folder cd newly-created-project-folder git init git add . git commit -m \"CCDS defaults\" # Use GitHub CLI to create a new repo on your account. This will prompt you for further details. gh repo create Your project is now ready to go!","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#prerequisites","text":"In order to get started, you'll need the following: A basic understanding of git and a GitHub account. A command line interface to run commands (often Terminal on macOS and Linux, and Command Prompt or PowerShell on Windows). An installation of pipx or pip .","title":"Prerequisites"},{"location":"getting-started/#setup","text":"Below, we walk through how to quickly set up your new project. This README provides two different setup flows for different expertise and preference profiles. If you would prefer, you are welcome to deviate from these setup flows and make choices as you see fit. More details can be found in the documentation for the cookiecutter-data-science template .","title":"Setup"},{"location":"getting-started/#standard","text":"Allows you to choose between using pip & venv , or conda for dependency and virtual environment management If you plan to deploy a package, Flit will be the default tool to use. This set up will be best for those who want to use the more traditional requirements.txt file for dependency management, or for users who want more flexibility for more complex projects.","title":"Standard"},{"location":"getting-started/#poetic","text":"Basic setup with a few extra steps to start using Poetry for all of dependency management, virtual environment generation, and package publication. This setup may be better suited for those who plan to only use Python code and packages in their project. The downside of this set up is that sharing code with other users takes an extra step if they do not have (and do not want to use) Poetry for dependency management. While Poetry can also be used for projects set up with the Standard installation as well, the Poetic set up best takes advantage of the unification that Poetry provides.","title":"Poetic"},{"location":"getting-started/#install-ccds","text":"The cookiecutter-data-science template team created a command line tool called ccds to run the setup. With pipx (recommended) pipx install cookiecutter-data-science With pip pip install cookiecutter-data-science","title":"Install ccds"},{"location":"getting-started/#shared-setup","text":"Run ccds from the parent directory where you want your project stored # From the parent directory where you want your project ccds Begin filling out the template prompts. These can all be changed later. Enter the project_name Enter the repo_name . Choose a name that has not been used for a repository on your GitHub account. Enter the module_name . If you choose to export your standalone code, this will be the name of the Python package. Enter the author_name . This can be your full name. Enter the description . Enter the python_version_number . As of mid-2024, using a version >=3.9 is recommended. Select the dataset_storage . For large datasets or CSVs, it is recommended they be stored on a cloud storage if possible. If you have details for those cloud storage providers, go ahead and choose the one you use and provide the credentials. If you do not currently use one of those providers but plan to do so eventually, you can select it now and use the default values for the credentials. This will create the convenient scripts in the Makefile , and later when you set up the cloud storage you can modify the details in the Makefile to reflect the relevant credentials (bucket name, etc). Eventually, we will recommend using the SCL Network Attached Storage (NAS) that is hosted in Cambridge for data storage. See the section ] Using the SCL Network Attached Storage (NAS) below for instructions on how to set up the NAS for data storage. If you will use the SCL NAS, select none for dataset_storage . For the remaining choices, choose a setup flow below to follow.","title":"Shared Setup"},{"location":"getting-started/#standard-setup","text":"","title":"Standard Setup"},{"location":"getting-started/#complete-template-setup","text":"Select the environment_manager of your choice. Select none if you wish to set up an environment manager that is not listed. Select the dependency_file of your choice. For pydata_packages , select basic if you would like the following packages added from the start: ipython, jupyterlab, matplotlib, notebook, numpy, pandas, scikit-learn. Select open_source_license . MIT is recommended if the repository will be public. Select docs . If you would prefer to use Read the Docs instead, you can select none and set that up independently. Select code_scaffold . It is recommended to select yes to include some useful boilerplate code. If you are an experienced Python developer, you may prefer to avoid the prebuilt boilerplate and select no .","title":"Complete template setup"},{"location":"getting-started/#create-a-python-virtual-environment","text":"If you used the Standard setup, you can use the following command to create a new environment using a recipe provided by the template in the Makefile: make create_environment You will then need to activate that environment using the environment manager you've chosen. See Create a Python virtual environment for more information. You can read more about the Makefile in the Working On Your Project section.","title":"Create a Python virtual environment"},{"location":"getting-started/#poetic-setup","text":"","title":"Poetic Setup"},{"location":"getting-started/#complete-template-setup_1","text":"For environment_manager , select none . For dependency_file , select requirements.txt . At the end of the setup, we will remove this and use pyproject.toml instead. For pydata_packages , select none . Select open_source_license . MIT is recommended if the repository will be public. Select docs . If you would prefer to use Read the Docs instead, you can select none and set that up independently. Select code_scaffold . It is recommended to select yes to include some useful boilerplate code. If you are an experienced Python developer, you may prefer to avoid the prebuilt boilerplate and select no .","title":"Complete template setup"},{"location":"getting-started/#set-up-poetry","text":"In the Poetic setup flow, we will manage dependencies using Poetry . With Poetry , you only need one tool to cover the following: manage dependencies, create local environments, and publish your package to PyPI.","title":"Set up Poetry"},{"location":"getting-started/#install-poetry","text":"Simple installation with pipx pipx install poetry See detailed instructions here .","title":"Install Poetry"},{"location":"getting-started/#initialize-poetry","text":"The cookiecutter-data-science template created a pyproject.toml for us already, but we will want to delete a couple sections from that before initiaalizing Poetry . First, delete the sections within the pyproject.toml for [project] and [build-system] . They will probably look something like this: [project] name = \"your-project-name\" version = \"0.0.1\" description = \"the description for your project\" authors = [ { name = \"Deniz Aydemir\" }, ] license = { file = \"LICENSE\" } readme = \"README.md\" classifiers = [ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\" ] requires-python = \"~=3.12\" [build-system] requires = [\"flit_core >=3.2,<4\"] build-backend = \"flit_core.buildapi\" After removing this text from the pyproject.toml , you can now initialize using Poetry . From within the newly created project folder run the following: cd newly-created-project-folder poetry init You will be guided through set up options, and you should chose the same project name you chose during the original set up.","title":"Initialize Poetry"},{"location":"getting-started/#configure-in-project-virtual-environments","text":"poetry config virtualenvs.in-project true","title":"Configure in-project virtual environments"},{"location":"getting-started/#install-to-download-dependencies-and-setup-environment","text":"poetry install This creates a virtual environment that stored in the project folder at /.venv . This folder is hidden to git thanks to the .gitignore file. If someone else wants to run your code, they can clone or download your repository from GitHub, and run poetry install themselves to have a copy of the virtual environment you are using.","title":"Install to download dependencies and setup environment"},{"location":"getting-started/#add-included-dependencies","text":"Let's add the helpers that come with the template to the development environment. More information on this is available in Working On Your Project . poetry add black flake8 isort --group dev If you chose yes for the code_scaffolding option, then run the following command to add the packages included in the boilerplate code to the whole project. poetry add python-dotenv loguru typer tqdm","title":"Add included dependencies"},{"location":"getting-started/#delete-the-requirementstxt","text":"In order to avoid confusion, it is recommended that you delete the requirements.txt file as it will not be used.","title":"Delete the requirements.txt"},{"location":"getting-started/#pyprojecttoml","text":"It will be useful to familiarize yourself with the pyproject.toml file. This file is the current best practice method for defining project parameters and dependencies. Poetry will populate and use this file as you add dependencies. Other build tools like Flit, Hatch, and Setuptools also use the pyproject.toml . Read more about pyproject.toml at Python's pyproject documentation .","title":"pyproject.toml"},{"location":"getting-started/#what-happened-to-requirementstxt","text":"When using Poetry, all your dependencies are managed in the pyproject.toml file. It is possible to use a requirements.txt to populate your dependencies in your pyproject.toml with Poetry. poetry add $(cat requirements.txt) Similarly, it is possible to export your dependencies from the pyproject.toml to a requirements.txt if needed. # Export dependencies to requirements.txt poetry export -f requirements.txt --output requirements.txt # If you want to include development dependencies as well poetry export -f requirements.txt --output requirements.txt --dev","title":"What happened to requirements.txt?"},{"location":"getting-started/#adjust-makefile","text":"In fact, in order to get some of our Makefile recipes working again, we will add those above lines to the requirements recipe. So now the requirements recipe in your Makefile should look like this: ## Install Python Dependencies .PHONY: requirements requirements: poetry export -f requirements.txt --output requirements.txt poetry export -f requirements.txt --output requirements.txt --dev $(PYTHON_INTERPRETER) -m pip install -U pip $(PYTHON_INTERPRETER) -m pip install -r requirements.txt It is important to note that with this change the make requirements recipe will create a requirements.txt based on your Poetry dependencies (both regular and development) based on the pyproject.toml , and install those dependencies using pip . However, if you are using Poetry to manage your dependencies, make requirements should not be used to install dependencies. Instead, the standard poetry install should be used. The Makefile is discussed further in Working With Your Project .","title":"Adjust Makefile"},{"location":"getting-started/#connect-to-github","text":"Whichever setup flow you chose, your final step will be to connect your local folder to a new repo on GitHub. In order to complete this step, ensure you have the GitHub command-line interface (CLI) installed. Then you can run the following commands to connect your local repository to GitHub. # Navigate to your newly created project folder cd newly-created-project-folder git init git add . git commit -m \"CCDS defaults\" # Use GitHub CLI to create a new repo on your account. This will prompt you for further details. gh repo create Your project is now ready to go!","title":"Connect to GitHub"},{"location":"publish-your-code/","text":"Publish your code The template described in this guide is a starting point for researchers who want to write python code, use Jupyter notebooks, and potentially publish the code they have written for their research. There are essentially three ways to publish your code so it can be reused (and potentially cited): Publish the code in a repository as is (with an optional DOI) Publish a Python package to PyPI Publish as peer-reviewed software (to pyOpenSci or the Journal of Open Source Software ) Code Publishing and Archiving Tools Tool Description Supports GitHub Public repository. If structured and documented well, easy to share and reuse code.\u200b Supports versioning and tagging releases. Zenodo , Figshare Formal research code archival.\u200b Creates DOIs for uploaded code. Zenodo automates this using a GitHub integration.\u200b pyOpenSci Community for open-source Python packages for scientific research.\u200b Provides peer review, endorses packages, and promotes community discovery.\u200b Journal of Open Source Software Journal for simple publishing of code as a standalone paper. \u200b Indexed by Google Scholar. Software must be non-trivial (multi-month project).\u200b Building a Python package These instructions will be useful if you intend to release your module as a Python package on PyPI. How to decide Would this module be useful to other researchers or developers? Releasing the reusable elements of your code as a package on PyPI makes it easy for others to use, and to have a formal way of releasing improvements over time. See more on python packages here . If you add your repo to Zenodo, then you will have a unique DOI (separate from the DOI of any papers you've published) that can be cited by others when they use your library. If you would like to have your package peer-reviewed and its visibility increased, you can consider submitting to pyOpenSci or the Journal of Open Source Software . pyOpenSci is a peer-review process that increases the visibility among the scientific community. The Journal of Open Source Software requires that the software be non-trivial by checking number of lines of code and the amount of time it took to write the software. More details on the submission requirements for the JOSS here . Publishing the package If you are ready to publish your module as a Python package, make sure that your git working tree is clean by running git status and ensuring that you see that there is nothing to commit. Decide on a version For every update that you release of your package, you'll need to decide what the version should be. Version updates should adhere to semantic versioning standards. More on semantic versioning available here Prepare release metadata Add a CODE_OF_CONDUCT. pyOpenSci recommends using the Contributor Covenant . Add a CHANGELOG, and update it with new releases. More details on good practices at pyOpenSci . Examples of the above can be found in this repository. Publish with Flit If you used the Standard setup, you will be ready to publish using Flit . Once Flit is installed, you should be ready to build and publish your package code. flit build flit publish Flit with prompt you for your PyPI credentials in order to automatically upload the package. If you have not created an account at PyPI , you will need to do that now. Flit will also use the version set in your pyproject.toml as the version for the package deployment. Publish with Poetry Poetry handles incrementing the versions for you, you just need to decide if the update is a patch , a minor update, or a major update. Any update that would break for existing users of the package is considered major . This template has an initial version of 0.1.0 . You can see the existing version of the package by checking the pyproject.toml file, or by running poetry version First version For the first time you publish a package, you need to create an account at PyPI generate an API token on your account. After generating the token, copy it and connect it to poetry using the following command poetry config pypi-token.pypi YOUR-API-TOKEN For all releases Run the following commands to publish a new major update to your package. poetry version major #if our current version is 0.1.0, this bumps the package version to 1.0.0 git commit -am 'version bump' #commit the version update to git git push #push the updated version to GitHub poetry build #creates the artifacts we need for the python package poetry publish #uploads the package to PyPI Create a tag and release It is good practice to create a release on GitHub to mark an update to the package. This ensures that you know what code is associated with what package version, and is required if you want to use Zenodo's automatic integration with GitHub. poetry version #prints the current version, say 1.0.0 git tag v1.0.0 #standard practice is to prefix version tags with `v` git push --tags #push the tag to GitHub After your tag is pushed, you can go to the Tags list on your GitHub, then switch to the Releases list, and create a new Release. Use the tag you just uploaded to create a new release with the same version. It is best practice to describe the changes included in this release in the description. Publish on Zenodo Follow these instructions to let Zenodo connect to your GitHub repository and automatically create a new DOI everytime you publish a new release version. Create a Zenodo account (either log in with GitHub, or connect with GitHub after creating the account) \u2060\u2060Go to Zenodo Github Integration and toggle on the repository you want to create the DOI for. \u2060In your GitHub repo, create a Tag to mark your release version (e.g. v0.1.0 ), and then create a Release using that tag. Now, Zenodo will create a new DOI every time you create a new Release on GitHub. If desired, you can add a badge to the README to link to the latest Zenodo DOI using these instructions . More information on using Zenodo (and Figshare) can be found at GitHub -- Referencing and Citing Content . Submit to pyOpenSci or JOSS You may consider submitting your code and package to pyOpenSci or JOSS. pyOpenSci Journal of Open Source Software More Resources The Hitchhiker's Guide to Packaging -- Creating a Package The Hitchhiker's Guide to Python -- Structuring Your Project","title":"Publish Your Code"},{"location":"publish-your-code/#publish-your-code","text":"The template described in this guide is a starting point for researchers who want to write python code, use Jupyter notebooks, and potentially publish the code they have written for their research. There are essentially three ways to publish your code so it can be reused (and potentially cited): Publish the code in a repository as is (with an optional DOI) Publish a Python package to PyPI Publish as peer-reviewed software (to pyOpenSci or the Journal of Open Source Software )","title":"Publish your code"},{"location":"publish-your-code/#code-publishing-and-archiving-tools","text":"Tool Description Supports GitHub Public repository. If structured and documented well, easy to share and reuse code.\u200b Supports versioning and tagging releases. Zenodo , Figshare Formal research code archival.\u200b Creates DOIs for uploaded code. Zenodo automates this using a GitHub integration.\u200b pyOpenSci Community for open-source Python packages for scientific research.\u200b Provides peer review, endorses packages, and promotes community discovery.\u200b Journal of Open Source Software Journal for simple publishing of code as a standalone paper. \u200b Indexed by Google Scholar. Software must be non-trivial (multi-month project).\u200b","title":"Code Publishing and Archiving Tools"},{"location":"publish-your-code/#building-a-python-package","text":"These instructions will be useful if you intend to release your module as a Python package on PyPI.","title":"Building a Python package"},{"location":"publish-your-code/#how-to-decide","text":"Would this module be useful to other researchers or developers? Releasing the reusable elements of your code as a package on PyPI makes it easy for others to use, and to have a formal way of releasing improvements over time. See more on python packages here . If you add your repo to Zenodo, then you will have a unique DOI (separate from the DOI of any papers you've published) that can be cited by others when they use your library. If you would like to have your package peer-reviewed and its visibility increased, you can consider submitting to pyOpenSci or the Journal of Open Source Software . pyOpenSci is a peer-review process that increases the visibility among the scientific community. The Journal of Open Source Software requires that the software be non-trivial by checking number of lines of code and the amount of time it took to write the software. More details on the submission requirements for the JOSS here .","title":"How to decide"},{"location":"publish-your-code/#publishing-the-package","text":"If you are ready to publish your module as a Python package, make sure that your git working tree is clean by running git status and ensuring that you see that there is nothing to commit.","title":"Publishing the package"},{"location":"publish-your-code/#decide-on-a-version","text":"For every update that you release of your package, you'll need to decide what the version should be. Version updates should adhere to semantic versioning standards. More on semantic versioning available here","title":"Decide on a version"},{"location":"publish-your-code/#prepare-release-metadata","text":"Add a CODE_OF_CONDUCT. pyOpenSci recommends using the Contributor Covenant . Add a CHANGELOG, and update it with new releases. More details on good practices at pyOpenSci . Examples of the above can be found in this repository.","title":"Prepare release metadata"},{"location":"publish-your-code/#publish-with-flit","text":"If you used the Standard setup, you will be ready to publish using Flit . Once Flit is installed, you should be ready to build and publish your package code. flit build flit publish Flit with prompt you for your PyPI credentials in order to automatically upload the package. If you have not created an account at PyPI , you will need to do that now. Flit will also use the version set in your pyproject.toml as the version for the package deployment.","title":"Publish with Flit"},{"location":"publish-your-code/#publish-with-poetry","text":"Poetry handles incrementing the versions for you, you just need to decide if the update is a patch , a minor update, or a major update. Any update that would break for existing users of the package is considered major . This template has an initial version of 0.1.0 . You can see the existing version of the package by checking the pyproject.toml file, or by running poetry version","title":"Publish with Poetry"},{"location":"publish-your-code/#first-version","text":"For the first time you publish a package, you need to create an account at PyPI generate an API token on your account. After generating the token, copy it and connect it to poetry using the following command poetry config pypi-token.pypi YOUR-API-TOKEN","title":"First version"},{"location":"publish-your-code/#for-all-releases","text":"Run the following commands to publish a new major update to your package. poetry version major #if our current version is 0.1.0, this bumps the package version to 1.0.0 git commit -am 'version bump' #commit the version update to git git push #push the updated version to GitHub poetry build #creates the artifacts we need for the python package poetry publish #uploads the package to PyPI","title":"For all releases"},{"location":"publish-your-code/#create-a-tag-and-release","text":"It is good practice to create a release on GitHub to mark an update to the package. This ensures that you know what code is associated with what package version, and is required if you want to use Zenodo's automatic integration with GitHub. poetry version #prints the current version, say 1.0.0 git tag v1.0.0 #standard practice is to prefix version tags with `v` git push --tags #push the tag to GitHub After your tag is pushed, you can go to the Tags list on your GitHub, then switch to the Releases list, and create a new Release. Use the tag you just uploaded to create a new release with the same version. It is best practice to describe the changes included in this release in the description.","title":"Create a tag and release"},{"location":"publish-your-code/#publish-on-zenodo","text":"Follow these instructions to let Zenodo connect to your GitHub repository and automatically create a new DOI everytime you publish a new release version. Create a Zenodo account (either log in with GitHub, or connect with GitHub after creating the account) \u2060\u2060Go to Zenodo Github Integration and toggle on the repository you want to create the DOI for. \u2060In your GitHub repo, create a Tag to mark your release version (e.g. v0.1.0 ), and then create a Release using that tag. Now, Zenodo will create a new DOI every time you create a new Release on GitHub. If desired, you can add a badge to the README to link to the latest Zenodo DOI using these instructions . More information on using Zenodo (and Figshare) can be found at GitHub -- Referencing and Citing Content .","title":"Publish on Zenodo"},{"location":"publish-your-code/#submit-to-pyopensci-or-joss","text":"You may consider submitting your code and package to pyOpenSci or JOSS. pyOpenSci Journal of Open Source Software","title":"Submit to pyOpenSci or JOSS"},{"location":"publish-your-code/#more-resources","text":"The Hitchhiker's Guide to Packaging -- Creating a Package The Hitchhiker's Guide to Python -- Structuring Your Project","title":"More Resources"},{"location":"working-with-your-project/","text":"Working On Your Project See the below sections for more information and best practices. Version Control Version control makes it easy to go back to previous versions of your project and create different branches for different lines of work. When connected to a remote repository like GitHub, this makes it so even if your local files are lost or corrupted, you can always retrieve the latest versions from the remote repository. If you have not used git and GitHub before, you may find it useful to download the GitHub Desktop Application . This provides a user interface to easily see what has changed and make commits. Many code editors (like Visual Studio Code) will also have some git tools integrated into their interface. It is best practice to commit and push your changes to the remote repository frequently. Include messages that are short and that describe the included changes. If you would like a crash course on how to use git , you may try Learn Git Branching . Make As part of this template, a Makefile is created with some initial scripts, called \"recipes\", that can be used to simplify set up and reuse of your repository. See Make as a task runner for more information. Virtual Environments During the setup, you will have created a virtual environment where your specified version of python and dependencies will be housed to run your python code. The use of virtual environments keeps dependencies and python versions from mixing between different projects, and makes it easier to quickly run someone else's project when needed. In your code editor, be sure to select the local virtual environment you created. It will likely be in a folder called .venv located within your project folder. Structuring your code It is recommended that you put the \"logic\" of your workflow into the module folder created by the template, and reference that module in any scripts or notebooks as needed. This makes it easier to... Share the core components of your code and eventually publish it as a package Separate different types of code conceptually Have someone else come in and review your code Publish your code as a package if so desired The module folder will be the one with the name you chose at setup and has the __init__.py file inside it. This file is what Python uses to determine that a folder containing Python code should be treated like a module. See Refactoring code into shared modules for more information and how to turn on autoreload so your Jupyter Notebooks always use the up-to-date code from your module. Jupyter Notebooks It is recommended that Jupyter Notebooks be used as a way to explore and visualize the data, and not where the actual data manipulation and \"logic\" happens. It is reasonable to experiment with more functional changes in your notebooks, but it is recommended that the data manipulation code eventually be moved to the module. One way to think about this is: could someone else ignore your notebooks and use only your module code to reproduce your results? If not, then it is useful to refactor your code to make that possible. If you plan to publish your module as a package, this will be necessary. In order to access your module code instantly without having to reload your notebook, add a cell to the top of your notebook containing the following code. %load_ext autoreload %autoreload 2 See Open a notebook for more information and guidance on a useful notebook naming convention. Dependency Management If the Standard setup was used, you can use the dependency manager of your choice (likely pipx or conda ), and the file of your choice as well (likely requirements.txt ). If the Poetic setup was used, you can add packages using the following command in the command line: poetry add numpy See more details on how to add dependencies with Poetry here . Development environment dependencies When adding dependencies, it is important to know if the dependency should be added to the whole project, or just to the development environment. If a package is being added to the whole project, it should be one that is required for the core module code to run. The easiest way to think about this is to decide, if I were publish the core modujle code in this project as a package, would this dependency need to be bundled in the package? If yes, then add the dependency to the whole project using the standard commands above. If a dependency is not required by the core module code and is used for work outside the module or for extra tooling, then you can add that package to your development environment. This makes it so when you are sharing your code or publishing a package, it is clear which packages are required to run your code and extraneous dependencies are not unnecessarily bundled in. Some standard examples of development environment dependencies include ipykernel and notebooks for Jupyter Notebooks, since notebooks would not likely be considered part of a core module to be reused by others. If you do not expect to publish a Python package or share your code with others, this distinction is not very important and you can add all dependencies directly to the whole project using the steps above. Development environment dependencies with requirements.txt If you use a requirements.txt file for your dependencies, you can create a second requirements-dev.txt file to manage development environment dependencies. A quick guide on how to do this can be found here . Development environment dependencies with Conda In order to set up different environments and sets of dependencies, see Managing Environments in Conda . Development environment dependencies with Poetry With Poetry , you can use groups to manage dependencies for different environments. poetry add ipykernel --group dev See Dependency Groups in Poetry for more information. Data Management Generally, it is important to keep your work reproducible. This means that given an initial dataset and your code, it should be possible to recreate all the intermediary steps and data outputs. This makes it possible for someone else to come in later and use your code to reproduce the results you've achieved, and build on top of them. Keeping the raw data that is retrieved form external sources immutable is also important for this reason. For this reason, this template uses the following structure within the data folder \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. Data Storage It is preferred that the datasets be stored on the Senseable NAS, or on some form of cloud storage (hence the dataset_storage option at initial set up). The data folder is included in the .gitignore file by default, so no files in the data folder will be synced to GitHub. This is generally good practice in order to keep large data files off of GitHub and out of your git history. GitHub will reject files over 100MB. If you think it is important to include the data files in GitHub, you can modify the .gitignore as needed. If your files are larger than a few MB, it is recommended to use git-lfs to keep your git history lean and repo fetching fast. Be mindful that storing the datasets on a public GitHub repository will make the datasets themselves public as well. See Add your data for more information on how to use the data syncing recipes provided with the Makefile . Using the SCL Network Attached Storage (NAS) [Pending instructions on connecting your data to the Senseable NAS] Secret Keys and Tokens API keys or access tokens you've generated for yourself should be hidden from git and not be uploaded to GitHub. If you upload them to a public repo, even if you remove them in a future commit they will be compromised and should be deleted. Instead, store them locally in a file that is ignored by git and thus will not be uploaded to GitHub. The standard way to do this is to use a .env file in the folder of your project and add your API key. The .env file is already included in the .gitignore of this template, and has already been created by the cookiecutter-data-science template. Open the .env file and add your API keys as needed: EXAMPLE_API_KEY=\"your-secret-api-key-here\" ANOTHER_API_KEY=\"another-super-secret-key-here\" You can use the dotenv package to access your token from your local environment. import dotenv dotenv.load_dotenv() api_key = os.getenv('EXAMPLE_API_KEY') # use api_key wherever you would use your API key normally If you chose yes on the code_scaffolding option, you should see load_dotenv() being called in the generated config.py file. That would be a good place to retrieve the environment variables to make them available to the rest of your code. Using SCL BIZON [Pending instructions for how to connect to and use the SCL BIZON workstation]","title":"Working On Your Project"},{"location":"working-with-your-project/#working-on-your-project","text":"See the below sections for more information and best practices.","title":"Working On Your Project"},{"location":"working-with-your-project/#version-control","text":"Version control makes it easy to go back to previous versions of your project and create different branches for different lines of work. When connected to a remote repository like GitHub, this makes it so even if your local files are lost or corrupted, you can always retrieve the latest versions from the remote repository. If you have not used git and GitHub before, you may find it useful to download the GitHub Desktop Application . This provides a user interface to easily see what has changed and make commits. Many code editors (like Visual Studio Code) will also have some git tools integrated into their interface. It is best practice to commit and push your changes to the remote repository frequently. Include messages that are short and that describe the included changes. If you would like a crash course on how to use git , you may try Learn Git Branching .","title":"Version Control"},{"location":"working-with-your-project/#make","text":"As part of this template, a Makefile is created with some initial scripts, called \"recipes\", that can be used to simplify set up and reuse of your repository. See Make as a task runner for more information.","title":"Make"},{"location":"working-with-your-project/#virtual-environments","text":"During the setup, you will have created a virtual environment where your specified version of python and dependencies will be housed to run your python code. The use of virtual environments keeps dependencies and python versions from mixing between different projects, and makes it easier to quickly run someone else's project when needed. In your code editor, be sure to select the local virtual environment you created. It will likely be in a folder called .venv located within your project folder.","title":"Virtual Environments"},{"location":"working-with-your-project/#structuring-your-code","text":"It is recommended that you put the \"logic\" of your workflow into the module folder created by the template, and reference that module in any scripts or notebooks as needed. This makes it easier to... Share the core components of your code and eventually publish it as a package Separate different types of code conceptually Have someone else come in and review your code Publish your code as a package if so desired The module folder will be the one with the name you chose at setup and has the __init__.py file inside it. This file is what Python uses to determine that a folder containing Python code should be treated like a module. See Refactoring code into shared modules for more information and how to turn on autoreload so your Jupyter Notebooks always use the up-to-date code from your module.","title":"Structuring your code"},{"location":"working-with-your-project/#jupyter-notebooks","text":"It is recommended that Jupyter Notebooks be used as a way to explore and visualize the data, and not where the actual data manipulation and \"logic\" happens. It is reasonable to experiment with more functional changes in your notebooks, but it is recommended that the data manipulation code eventually be moved to the module. One way to think about this is: could someone else ignore your notebooks and use only your module code to reproduce your results? If not, then it is useful to refactor your code to make that possible. If you plan to publish your module as a package, this will be necessary. In order to access your module code instantly without having to reload your notebook, add a cell to the top of your notebook containing the following code. %load_ext autoreload %autoreload 2 See Open a notebook for more information and guidance on a useful notebook naming convention.","title":"Jupyter Notebooks"},{"location":"working-with-your-project/#dependency-management","text":"If the Standard setup was used, you can use the dependency manager of your choice (likely pipx or conda ), and the file of your choice as well (likely requirements.txt ). If the Poetic setup was used, you can add packages using the following command in the command line: poetry add numpy See more details on how to add dependencies with Poetry here .","title":"Dependency Management"},{"location":"working-with-your-project/#development-environment-dependencies","text":"When adding dependencies, it is important to know if the dependency should be added to the whole project, or just to the development environment. If a package is being added to the whole project, it should be one that is required for the core module code to run. The easiest way to think about this is to decide, if I were publish the core modujle code in this project as a package, would this dependency need to be bundled in the package? If yes, then add the dependency to the whole project using the standard commands above. If a dependency is not required by the core module code and is used for work outside the module or for extra tooling, then you can add that package to your development environment. This makes it so when you are sharing your code or publishing a package, it is clear which packages are required to run your code and extraneous dependencies are not unnecessarily bundled in. Some standard examples of development environment dependencies include ipykernel and notebooks for Jupyter Notebooks, since notebooks would not likely be considered part of a core module to be reused by others. If you do not expect to publish a Python package or share your code with others, this distinction is not very important and you can add all dependencies directly to the whole project using the steps above.","title":"Development environment dependencies"},{"location":"working-with-your-project/#development-environment-dependencies-with-requirementstxt","text":"If you use a requirements.txt file for your dependencies, you can create a second requirements-dev.txt file to manage development environment dependencies. A quick guide on how to do this can be found here .","title":"Development environment dependencies with requirements.txt"},{"location":"working-with-your-project/#development-environment-dependencies-with-conda","text":"In order to set up different environments and sets of dependencies, see Managing Environments in Conda .","title":"Development environment dependencies with Conda"},{"location":"working-with-your-project/#development-environment-dependencies-with-poetry","text":"With Poetry , you can use groups to manage dependencies for different environments. poetry add ipykernel --group dev See Dependency Groups in Poetry for more information.","title":"Development environment dependencies with Poetry"},{"location":"working-with-your-project/#data-management","text":"Generally, it is important to keep your work reproducible. This means that given an initial dataset and your code, it should be possible to recreate all the intermediary steps and data outputs. This makes it possible for someone else to come in later and use your code to reproduce the results you've achieved, and build on top of them. Keeping the raw data that is retrieved form external sources immutable is also important for this reason. For this reason, this template uses the following structure within the data folder \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump.","title":"Data Management"},{"location":"working-with-your-project/#data-storage","text":"It is preferred that the datasets be stored on the Senseable NAS, or on some form of cloud storage (hence the dataset_storage option at initial set up). The data folder is included in the .gitignore file by default, so no files in the data folder will be synced to GitHub. This is generally good practice in order to keep large data files off of GitHub and out of your git history. GitHub will reject files over 100MB. If you think it is important to include the data files in GitHub, you can modify the .gitignore as needed. If your files are larger than a few MB, it is recommended to use git-lfs to keep your git history lean and repo fetching fast. Be mindful that storing the datasets on a public GitHub repository will make the datasets themselves public as well. See Add your data for more information on how to use the data syncing recipes provided with the Makefile .","title":"Data Storage"},{"location":"working-with-your-project/#using-the-scl-network-attached-storage-nas","text":"[Pending instructions on connecting your data to the Senseable NAS]","title":"Using the SCL Network Attached Storage (NAS)"},{"location":"working-with-your-project/#secret-keys-and-tokens","text":"API keys or access tokens you've generated for yourself should be hidden from git and not be uploaded to GitHub. If you upload them to a public repo, even if you remove them in a future commit they will be compromised and should be deleted. Instead, store them locally in a file that is ignored by git and thus will not be uploaded to GitHub. The standard way to do this is to use a .env file in the folder of your project and add your API key. The .env file is already included in the .gitignore of this template, and has already been created by the cookiecutter-data-science template. Open the .env file and add your API keys as needed: EXAMPLE_API_KEY=\"your-secret-api-key-here\" ANOTHER_API_KEY=\"another-super-secret-key-here\" You can use the dotenv package to access your token from your local environment. import dotenv dotenv.load_dotenv() api_key = os.getenv('EXAMPLE_API_KEY') # use api_key wherever you would use your API key normally If you chose yes on the code_scaffolding option, you should see load_dotenv() being called in the generated config.py file. That would be a good place to retrieve the environment variables to make them available to the rest of your code.","title":"Secret Keys and Tokens"},{"location":"working-with-your-project/#using-scl-bizon","text":"[Pending instructions for how to connect to and use the SCL BIZON workstation]","title":"Using SCL BIZON"}]}